{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emergence of Linguistic Structure from Information Processing Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tikzmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext tikzmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext tikzmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human Language as an Efficient Code\n",
    "\n",
    "Why does human language have the particular form it does? Here we postulate that the phonological, syntactic, and semantic-compositional structure of human language arises as a consequence of real-time information processing constraints operative in the production and comprehension of language by humans. \n",
    "\n",
    "The idea is that human language is an efficient code for communicating meanings in the information theoretic sense, subject to certain constraints. When you optimize to find the most efficient code subject to the information processing constraints, you get multiple solutions (or possibly multiple local optima). Those multiple solutions are the human languages: English, French, Chinese, etc.\n",
    "\n",
    "Let's think of a language $L$ as a stochastic mapping from a **message** $m$ to an **utterance** $u$. Let $M$ be a distribution over messages, and $U$ be the distribution over utterances obtained by drawing samples $m \\sim M$ and running them through $L$. The useful information conveyed by the language, measured in bits, is the mutual information between utterances and messages:\n",
    "\n",
    "$I[U:M] = < \\log \\frac{p(m|u)}{p(m)} >,$\n",
    "\n",
    "where $<x>$ represents the expectation of $x$. Therefore the best languages (which convey the most information about messages) are solutions to the optimization problem:\n",
    "\n",
    "$\\arg\\max_L I[U:M]$.\n",
    "\n",
    "In the real world, we don't want to optimize only information conveyed; we also want to minimize the effort expended using the language. Therefore we solve the constrained optimization problem:\n",
    "\n",
    "$\\arg\\max_L I[U:M] - \\lambda C(U)$,\n",
    "\n",
    "where $C(U)$ represents the expected **cost** of an utterance---the joint effort involved in producing, transmitting, and comprehending utterances. The scalar $\\lambda$ is a trade-off parameter specifying how many units of cost are worth it to transmit 1 bit of information. Typically in information theory, cost is measured in utterance length. Here we consider a more generalized notion of cost.\n",
    "\n",
    "Our proposal is that when the cost function appropriately encodes the information processing constraints that affect human communication, the solutions to the equation above will yield distributions over utterances given messages that resemble human languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Problems Inherent to Language Use\n",
    "\n",
    "By information-processing constraints I mean constraints on on-line language production and comprehension. I'll focus on the production side because I think production constraints are more severe than comprehension constraints. \n",
    "\n",
    "The main problem in language production is that your brain has to coordinate the muscle fibers in your vocal tract to produce an extraordinarily complex sequence of motions which encode an abstract meaning. The problem of coordinating a system with multiple components to produce predictable behavior is called a **control problem**, studied extensively in the fields of Control Theory and Robotics. \n",
    "\n",
    "The control problem involved in human language production is enormously complex and we are not especially good at it, possibly because spoken language evolved recently. Evidence:\n",
    "\n",
    "* Disorders of speech production, such as stuttering and phonological speech impediments, are extremely common. \n",
    "\n",
    "* Even among the most fluent speakers, errors in speech such as false starts and disfluencies are common.\n",
    "\n",
    "* Fitch (2018) surveys the recent literature on evolutionary adaptation to language in humans, and finds that the only physiological adaptation to language is the existence of direct neurological connections between the motor cortex and the vocal tract. These connections are absent in all other animals. The existence of these connections, and the fact that they are the only observable adaptations for language, suggest that the motor control problem involved in human speech is the hardest part of using language, and is thus the source of the most major constraints on human languages. \n",
    "\n",
    "So I think the appropriate constraints on human language will come in the form of constraints on the motor control problem involved in speech production. There is a limit to the complexity of sequences we can produce with our vocal tracts. \n",
    "\n",
    "It is also possible to formulate language comprehension as a control problem.\n",
    "\n",
    "#### Control and Information Processing\n",
    "\n",
    "What do control problems have to do with information processing? An exciting synthesis has recently emerged in the control theory and robotics literature is that the complexity of a control problem can be considered information-theoretically. \n",
    "\n",
    "Suppose there is a distribution $G$ of **goals** that a robot wants to accomplish, and that the robot has a **action policy** $\\pi$, which is a stochastic function from goals $G$ to **actions** $A$. That is, the policy is a conditional distribution of actions given goals, $A|G$. Then the general complexity of the control problem is given by $I[A:G]$: the amount of information about the goal required to select the correct action. We imagine that $G$ and $A$ are two computational nodes, and $G$ is sending information to $A$ at a particular rate of units of cost per bit of information. A policy that requires a huge number of bits to be sent from $G$ to $A$ is complex. A policy that only requires a few bits to be sent from $G$ to $A$ is simple. \n",
    "\n",
    "Suppose an agent is trying to find a policy $\\pi$ that maximizes its reward under the complexity of the control problem. We can formulate such an objective function for $\\pi$ (to be maximized) as:\n",
    "\n",
    "$ J(\\pi) = U(A,G) - \\lambda I[A:G]$,\n",
    "\n",
    "where $U(A,G)$ is the expected **utility** of actions given goals, and $\\lambda$ is a scalar trade-off parameter. Maximizing this function, an agent might sometimes choose a policy which is sub-optimal with respect to the utility, but which has a lower information processing cost. The very best policy for maximizing utility might have prohibitively high information processing cost. \n",
    "\n",
    "The formulation above yields a number of results which are exciting from the perspective of predicting the behavior of humans and animals. Genewein et al. (2015) show that policies maximizing the function above generally induce **hierarchical** representations of the environment and of behavioral goals, a noted property of human representations. Van Dijk and Polani (2013) show that agents following the objective above tend to develop behavior which is **ritualized**, meaning that they tend to repeat the same actions in the same environments, even when these action are sub-optimal with respect to their current goals. They also find that agents under this objective will typically only read a few bits of information about their goal per timestep: only those bits that are important in their current state. For example, in a task of navigating through rooms, while an agent is in a room it only reads the bits from $G$ that tell it which door in the room to head towards.\n",
    "\n",
    "In general, action policies resulting from the minimization of $J$ above have the property that they lack flexibility---they prefer action policies that involve performing the same actions regardless of the current goal.\n",
    "\n",
    "Here is an example. Suppose you drive home a particular route every evening. Then usually when you are driving home, you are deciding where to turn based on your particular local goal $g_\\text{go home}$. Let's say $g_\\text{go home}$ can be represented as a series of bits $01111011101$. Every time you decide where to turn, you have to look at some of those bits in $g_\\text{go home}$ to determine your action $a_\\text{turn left}$ or $a_\\text{turn right}$. \n",
    "\n",
    "Now suppose that you are driving a friend home and are going to drop them off at their house before proceeding to your own house. Now when you are driving your friend home, you are under the influence of a different local goal than usual: $g_\\text{drop off friend}$. Suppose you're deep in conversation with your friend and not paying careful attention to your driving. In this situation, you might find that you turned right when you should have turned left. Usually, those wrong turns will be the ones you *would have taken* if you were following $g_\\text{go home}$. In this case, your action policy lacked flexibility: you should have read a bit from $g_\\text{drop off friend}$ to determine you should turn left, but instead you turned right because that is what you usually did under the much more frequent goal $g_\\text{go home}$. \n",
    "\n",
    "If you reflect on everyday small mistakes, you will find that they often have this flavor: there is something you should do given a very particular goal, but instead you end up doing something that you would have done under some other, more-frequent goal. \n",
    "\n",
    "In general, maximization of $J$ will result in policies where:\n",
    "* When the policy selects actions that are sub-optimal wrt the utility, those actions are usually actions that the agent would take under some other more frequent goal.\n",
    "* Action policies with a high degree of similarity across different goals will be preferred.\n",
    "* The policy will be less efficient for less frequent goals.\n",
    "* The policy is soft: it does not involve hard rules determining behavior, but rather probability distributions. \n",
    "\n",
    "This \"inflexibility\" of complexity-penalized policies seems correct for modeling human language. Human language is about 60% redundant per word in terms of information, indicating a great deal of repeating substructure. We also describe things in similar ways even when they are not so similar. When I say \"lift a cup\" and \"lift a barbell\", the actual physical actions involved are extremely different and they are only similar at a very abstract level. But by categorizing both actions as \"lift\", I get to re-use the part of my speech production action policy that produces the phonemes /lift/. Some precision/efficiency is lost, but at the gain that the language is easier to produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control problem of language production\n",
    "\n",
    "In the simplest analysis, language production is a dual control problem. At each timestep, you must decide which actuators in your vocal tract to move next. This decision is based on two factors:\n",
    "* The **articulatory goal**: The word you are trying to say, or the message you are ultimately trying to convey.\n",
    "* The **speech context**: What you have already said so far.\n",
    "\n",
    "For example, suppose you're trying to say the word cat. The space of possible actions you can output is the set of all articulable phonemes. You have to produce three phonemes in order: /k/ + /æ/ + /t/. Suppose your articulatory goal is fully characterized by $g_\\text{cat}$, meaning your goal is to produce this particular word. And suppose you have said nothing so far.\n",
    "\n",
    "In the first timestep, conditional on $g_\\text{cat}$ and the empty context $\\epsilon$, you must choose to produce /k/. In the second timestep, conditional on $g_\\text{cat}$ and on the context /k/, you must choose to produce /æ/. You have to condition on the context because if you only considered $g_\\text{cat}$, you might produce /k/ again. (And note that repeating sounds, especially at the beginnings of words, is an extraordinarily common speech error, and is characteristic of stuttering.) Now finally, conditional on $g_\\text{cat}$ and the context /kæ/, you must choose to produce /t/.\n",
    "\n",
    "Letting $A$ be the set of actions (phonemes to be produced), $G$ be the set of articulatory goals, and $S$ be the set of speech contexts, an agent must learn a policy $\\pi : G \\times S \\rightarrow A$.\n",
    "\n",
    "The control problem is schematized below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADwCAQAAAB/a5oaAAAAAmJLR0QA/4ePzL8AAAAJcEhZcwAA\nLiMAAC4jAXilP3YAAAAHdElNRQfjAxEGKBHAYXtMAAAKeHpUWHRSYXcgcHJvZmlsZSB0eXBlIGlj\nYwAAWIWdl22SZKkNRf+zCi+BTwktBwSK8P434ENWd0+33TNjOyuIzHoPhJCu7hXpn+7pH3zqUEn5\nfVbJPz7167ccudq1jtq115rHHDZWzX/2SVKkadPcy8gjd//TmX/xCXZ9Hv1w57R6/h9DH4/+x/lL\nugxt0r758u0E6omDZa3aP/8XnV8v6lQlQvn78/XNTulSZf/xfPfvzxPh/ITx63+fPxboz8+P/2Ho\n5+eRfzbUycyXqzV/7TCzY+j3z/9kfvr1zN8/tfbyDiwmwvtJ+puECMdV4Y2MmrV9h0a33lJvTCJK\nbxrLXMbvo/x3ptN/2v5vTf/6+dv06zv6/JYlPh0/yJqxkYkKb9j+efTXcWi15VYakP1diUQbD8zl\nu0eliPvf1dL3z+/mSaz6OVqb8RWHZr+fWM3e99b5mVfmWf8+72Oo9m/IjfmJxRYPED/Ikvxi8Uek\n8jP4FsUDI8MwVC6m2isLBkVL0jJ1k9v+WtlZ9HbqLBo8GHg3WPOwJ/MRDil5R1N9RQc8CdrEg4mB\ndxLDgGHAMLAwsHi4MLDrOySDNc4aZ41vDD3mOCw6GGBevvy+++M1TMPY5OX9KeOQmsYwRuRSB4P3\nDY9Km4zLUXkIsRWyXnC/YKMIi4V3yju8LhMjeFyMOXhboNaCp2UXDG1+4GJxvg/fh+/L9+U7WBCL\n4mwMh4Y741AvwghCO8lUYXA0qpnBS3avykNlIdmr8+ZqTCTHdWFks5gNq29yMnJ9OSIEFei0l/6W\nN+AVklXyo9rGLtQbI3KDd5rwTvFJL4Djf+N/jDcC3zb/u+Z2Goaw3K7nFka2hcJpmfphHApr594n\nCEAXSHfH447BPp36XqCCd3javafcDxOIyYNJjwvUTh7F8yAboy2gA9zHzIOjD6AygMjAq7EYG+lx\nxhkJbPGDNH/+OKJUzY/IBU+E7ImsLLrBnmexk2VFFn84LFluo9DgnKwpK5hQdtd24IzIVD4Y7VnZ\nWakxJdC6eX4gLjbVmFDrBr+RJ1Uwu+Q5VgLMN084ZOLuXAtg8z+L5tU8AaMBXgN4xjGNjUx6NrVs\nk98g3gi4eaRs7GIsWKXkxbEWni0gsTjSomwWEFhkaBGLhZqseHnmD0Ld0MWGk7ZQtJu620ze+5UP\n3wR+k0EvQLCu7EDBh2cH3Q62fGn2V2YA1zF63l9Fsk9/pbbyIS6HiQfIH2fC4TfxuMDhgr5L9i7H\nuhr52qYcJV9CcO+lLPEoOH8A84AaAlQHsYrdUOPIcV95E6VKBjqMK5xfcdk2bvP86FtYKOTE4LsH\nfHtKmV7KIlpupdzJ4bRQV6X2Uar0QumUulqpzriQ+SP0ykDXCuIIATAWmPYBEQxKU0qn8Ho3RHqV\nPnfp60AOlz0hh1LLaHRCQwqyAVnsVMY+hVO9ait0CEVYLOJFZhTZFUd5Fqso1KC9FJVBr2FF1y1g\nq2homQVDFHqZvJxzlbkCYuc3Cz+Uw5FMdjFOahvonkNj0suqqyxCs1Sho1uARiqLgOJ42W2XzTE3\nBjee7LPKYyAgUHzwrbs48XH34gT4QFqHKj76KMwSHUsrB2O3SLl4d4nJtV4ugLrXSpCNaLeE8Jvn\nsaPEXfVDpcSewqvAPIE6SAOyI1UQ4OTQbL+Ipt/Kqlqr1jpGrZOfK2o9B81ZFd6qcFVt1mvzmmqL\nx5ZRez90Eo7G7drPetVVB5OHMJD64YxAyetTc8bU17xVuZP84pF2q6pUGQb0OOp26mxB8wdsFo6c\nXu2JLUYJPKJ7KmxC8eAgbcxio0X6oeOARGrdTaBlq5uJIKI+avNm1eVWx6AfhTO9HuJyVOph43PB\nJaC53VPFMzhcKzVTOSBcvmpYqcFRImCuNmAvim9RvWdTB0C5kz5CVDbfURu+pValtWob3u+Nma1B\nzk2jtT1bI2UdX+mRWrfb+pl0Mq0N+HlM+jOvbcShODQ1UYK/bpNriEVv+kTDvOnRNktvNCBtTm/T\n52tWPkkyNrLNwQO6w8zSnhpHRVmiceK2BViu1fadZFQbbV9zjuS3tVNro1oaOG0wTLso0mXTiyLB\nJIn8lBZMoFlqcSvK2KjZ/ijykQ+hBYVCRS8HpRd/UCpcr3sQUCUe7KSHrhaJ6shhpx3tc3Uq/JEG\nUkZDDSmPc+nSa389oazdJZA2oqS6gR0Sh2BNJLtTyH1Cj0blmBDTZZ1OhrxoX3o6jvQN/Dfx3hje\neE39dZLafa8OpDqzUj9GMo73SxNw5Xag8KWVtMrEssd5Qg9hKxex/ageqkAKoYNBYQ5AMCqXGlCn\nA1ob5BFhXYOAjd6xSmPZz6bK5hjKQZ1qgVcFaZVlgy55EIyhVBIqnsYEglPPmL6HwTImBuEheVnH\nYtlajBhjE7VtjIvNxoDE/Mg4eHt0pnHcBtQ0rvi4+wwoHwUvAwGg1cIJLqwIG844/MubBY3iWCWi\n1bjkoOCPswV0SUNb+ku6denXQA9bGUV+VYTflKBQ5YKsixoYZg6FLaizzOvyLjVitsTiIWVy9KBH\nUNnsvBffEfip4otrK+J+6DHONqFW5cqW66CBiAdHk4DTaccQevqWS24AfLGh9AgkmGpeOEIH2YgE\n9QdC+9fd0skSZEPnrsQmvXOpwOwSXD9pgnQ3BAah4Lo+mWx1qU3ahgtrcbEksTQ5XeF33dQRvKo+\nMeRPVbjfUEP6+tcLBV4mwA50MF3j0mV1LrtrvpZiolGz+IFEMkwHAUeHEjRNqhT9PBOsz34pdhaN\ntemOXnQrgeGW9c5kMbE4pxhkcKdB2mb4GndSlmkuXxOpn8Rw7vDpAmPw7EBdhzUnYt5Pcu6Mhmwa\nfTO9G+0a3QbSQvNZ1kyGfEDay9DyVywGl0A59FSToqNOxggbbp8yJL1GB2UE04iDze42N47VnvAu\nm4UDgmnrAGq4fq8wZNCcOR5qB4ShQobu2V0XtBwOui2CFk9ob89MdAiKtAr0zjBZEDSFz0ApO1VF\nmVOAc43FXrQqBGCBGVB2F16tiZBM2uMFwTLFaGZ8LUQfRVmbMtvXkHRfTid4Or0IWn7RjovsP/zi\n0X53O0qSrmulTRuyy0GwOorvMH0j9utyQurUqOTS9piL/gy/1TbEBujmxhtKm/I+3Gbgo20shqX3\n2gNLlx8PZ2W77dfw7ENrywmgcTgtUH6UNIKmklYyXzoKURqHlmCZQPWQBIikHS4DtP3QrY++ORlo\n6Fz9nRtHfw0J+GjH53ZHP9jLaFCmE4vksIVvbrFYcg7iKJbDZwiH+H2326YeHIDbzMmbtq05h6EN\nbXG4LR3Y/iA3iTgafkBE/Z5xiNYYRw4sjj3icKYgixdsCg0xeSddZ8Um9jS/3EJ8LtqvnA4zkHA/\ntDwnaA9icbNBLvPmcee64/Q3Axk7GyfbhbsuMnJ7OFUIzedzxSRd+OICACSRNmA7PRbYPyQUUl0X\n0oRcNvGGWi997z3mdAnzktcbKF84ffSYie57RKFfKBH0MoSkWEBJ0REQdAe2hnvPDZET8pJGozmZ\nMwEdrQ4loAGzpFi08ls1yCeFMomgxaFGbt9xj8ORlG1E+hftkQTIS62KtQAAEH9JREFUeNrt3dl3\nVed5gPHfkY6EJiSBQBMzEiBjBg8Y2/GAcTxi147jqYlX6uW0WStN77pWLvon5KIXWblIly8aN3Hb\ntImdGMcxdozjIWAbz4R5nhEzCNAsnV5womIQcI4kpAN5H27E0T57a33P/r73/aa9CYIgCIIgCIIg\nCIIgCIIgCIIgCIIgCILsyY8iyGHGqFSCrn5/mydfQiqTEyWiLHOMYqWKFRmlykRlehy1ywktWrSd\ndVyZ6bodciiTkyajXHOGhIRC17lekymmqpInISWlw2rv+5M1Tve1wk3+2RavOZxJfQ7NuUKeORaa\n7VrTFNhqk5OO2KdNqXplbnWbd71hoxRmesxtNjmQ2clDcy5QYIK5FrvddJ32Wu9D2x121H6tSkww\nzU1udK8yr1ltqkc9rNQuezKLzaF55GtxoRnu9bSZUvZ632tWOKUXKSm02mKLFeb4gSUqtHvC02ba\nYp/uKMArIx6P9k2/sEu77Z53l6lGX+DYEnP9h63etF2X435lYaaXido8slR7whK3GmON33jVFxfo\nPEGrNd41w21K0GaLE6E59yk0yT2+Z44en/mVl226xDdS3jPZfLLVHIxc2jXLjxzQIeVDz6nLcAxj\niT16pax2q1GZJwDByDDadzymWqEvvOR1zZnlzA5Z57SUA/bqCM25HpOf9LBGKUf8ztKMJdNqhzZH\nbHUy88tFbB6ZBvtm/6hJwikfW2ZjFt/tckKPo7bqyabXFgw/d3hCo1Fo9jMbBtANO+mQ3qjNuUvS\nREvcpwQHrfCxowPQfMCWi3S9QvOIU+w2t6kF67NIvc6WnLLfpmxGwELzcDPW/aY7M5C52rs6s/x+\nSruDdjmYXRMSDCcV5lugGl0+tcrBbCIsOOlz7T7JNlIEw8lEt6sBPdbYkrVkTvjEBkdCcy7T6E4l\noMNauwdwhjY7B5L3BcNFQqVrXK8QKUd9Zu9wXTr6zcNHnmmmKwRHferwcF46GL7aPNWEdPfppM1a\nQ/PVWZtrjU/PQ7U7kHVXKjRfMX3myvRPp+0YztocKdhwVqk61X3dojVOXfToYqOVXnAOukero5lP\nXoTm4YzN41Skf263/6KSEmZZYHK6te3VqzetPCFfnmPWeucry/NDc85QeFZtzLvELTHHN82RLyVP\nUknaVEqnNinbFFgZmnOTjr4me98lxr96LbdWGXqVmevvNYFOn/p3+7Q43LcDIzTnFCmHHDMGrZeI\ny7Df/r6fd3gwrXmzt7ya/VK/yLSHj1770/NKxUqzSpN7+ur+OquyqcWheSRq8xHHQaX6LLYcl5nT\n1xHbaesApjtC87Bq3tE3WVFqTMZlX6whvROj3W67s16GEJqHudFea2N6zUepaYoz/F6JmcrRZY/9\nukNzrmveY61mPRhrQV8f+lJUuFYlOqwf6HRHaB5eNlvmJOo9bHyGfe2JGhSjy450bA/NOc4mL9um\nV5nZ7lCXwTeu83A6AeuwMbNHUJxPPGJmuAdIjhpjnPGKFDll9yVGsiZ5ymNKJSUc9LO+2B6ac5xO\naxSZpFKDUic0X2AnVJ58Nb7jPpWOKVJoi59lt54zNI9kt6rNLifNNM4EU5U55Vg/dbTavb5vsXV+\naatG+Vak4/oAiMHOkek/vyJlsVstVGmu1bY6rFNCAkll6jWYqsQKy3yqTpPJ1mW+A/Jc4rlgI0WB\nxZa4VbUyLbbZp0eedj0oM1WRnZZbZrdeSY+bYK0VGYyFh+Yco1iRGjebYZI6Y9Vgh1MO2GWPz23X\nqbVvOCWpU8dABjpDcy5Qo0KFcqVKcFKnViecGN61YkEQBEEQBEEQBEEQBEEQBEEQBEEQBEHQH7Hk\nL9dIZv7mx9B8pVJgkRuVOTy0b5iKlZ25pvkBTd61QXtovpqD6HVu0nzWU0qGhNhDlWsk0qu1Q/NV\nTmqoE7DQ/FdCaA7NQWgOQnMQmoPQHITmIDQHoTk0B6E5CM1BaA5CcxCag9AchOYgNIfmIDQHoTkI\nzUFoDkJzEJqD0ByE5tAchOYgNAehOQjNQWgOQnMQmoPQHJqD0ByE5iA0B6E5CM1BaA5CcxCaQ3MQ\nmoPQHITmIDQHoTkIzUFoDkJzaA5CcxCag9AchOYgNAehOQjNQWgOzUFoDkJzkJskowhGlARIXfKY\nVGi+chml1mxHbHXkgsfUmyHfJs26Q/OVyRg3e1KLld63S/t5vy/VaLGFeMGR0HylUm66+SZZaJpX\nrHfqnJtgnufcbryjfj+4LCo/ynoEOW2nL+VpcL0mx2xU5CkTrLNcwsP+xR0Oe8lPrHRiMPE5avNI\n0m67Zofs8Yjbdeu1SkpKnir3ec48a/3WazboGtyFojaPNN322Cmh2mw1DrtXpWZ5nnadj73oFZv0\nRjFdHaMX1Z7xtkNWOuqAT2x12FKLjBmakY2ozblAymnN8k00V4WUUvVWetF7g4vIoTn3aHUETcoV\nYYMXvX6R3nRovmI55pAxJiu3zwtetTuK5Oqk0Hx/cNB/mZkeBo3afBXS44RWn3nLl4MZ8zqfRJRt\njjEa7bqHJvUKzblKJnNWQRAEQRAEQRAEQRAEQRAEQRAEQZDjJC7jWWMybahK88xmuUGU5+VYPZJU\npU6Zrn4WkZcpGOzS8r86Kn1dvR4nBqNk6ChRrU6dWuOV63LIPvvt0qwVFGkwyX5fhrksqPA1f+eQ\nV+0aec1J5ea6wXXmGSMhhQKtPrfKB77QKqXRd1R6KzRnxXWedr+91g9Oz9BEj2mecb9yW7xirb3a\n9ajXZJIn3eXfvKPaIx7zrgNhLotyLXCT+5RIDG73xVBoHu1Oj1noqNettMEeLWC9dSY64AbPKtfg\nKZPssjPsZUyB231NrV6lStJt5Ajdb2M87DeOed/3Te73JnjWR5baIOWYbysMexmX7WQ/sUtKr9N+\nNJLLM8t93XItVlii6gJ/SIMf2iKl01qLw17GVHnSGqn0v58ORvNg99vN8U8W2ON1nzlygUZlhzcd\nxynrHAt7GTPVk8b3lemgmuzBaZ5mkTsV+dCrDl/wqB67bdaq1aZzHrsQXJgat7jWJtv7gl/9wEc5\nBqM5zy3uVWa/VZfYDNJri4Pa7QjNGZJvkds0e8uG9CdjTBt4XjPwTDuhzAIL9fqs7467sObDTkvZ\n7vQVnRIN1wBuwmh3qfVLe80aihMOvDYXajRdqW5rLjk+k9Kty3Fbr+DanNRowjA9q2W0+0223R/P\nCoVFxgy80R74n12kSQ1STvXzRKvz78/j9l8wSbsSKPaoDu9YP7R7FfttsBs86aQ37dDZ1/6VqR64\nreQg7u56ZchXq/KSkhP22HpFPyqlyCPGK7fnsvcW6t3mOs97U7edfaOGxcaORAqWkCeBPBNVZXCd\nI3bruYI199iuyn0ajbrMV5rvXh/62DGpr2QDI9RvPjMPWmCB+UovemSHlV7xwRU9BdlumW3medzU\ny3iVPDPcbrzfW5v+pE2HFEpHptHudVoXkqa5x3ZfOuz0BeJWh4+v+C5Oh3fMNsGjmrXbdZmyjCL3\nm2u7DxxMf9LikDr5g9M88GUF+cpdb1p6vGaBMvk6tcuXJ+8qXDmSctpJ+W42B+vTdWyo63KtHxjl\nf32pI/3ZNa4zTr7Ttlju5HDX5laf9M02JU33tLsd1myLPbZaN3QPO8oh0X/2klqLfVvS8/YN+RVq\nPKLcKsvPGl9IpRPXAsUDD7H5g7q7a0xRk74PK9RrdI2pGs3QoEGNQi1XdNp1Lt32O6Jco1kKtTuh\nc0jr8kLfs8vvrD6rpWgwz0RJScctdXy4azO8Y7yxas86yygzzECvozZ4z1Lr07PP2TNGec5NW/ba\n53MNbvZDU/23zxwfsht5sgUm+x+ffOXT45r1okDdSKRgsNWvdXjWNee1CnnGWmC66/3EHwY0oJDv\nCU+akoNxvkyFhFGe0OBFS4dsPcwdFnjDR46e02oafJdqcJp7bPeyPW4wyxRTVXxFdJF6pVoVWDqg\ns9eaqzZnG/ACBZpMUzxEZ5vlLmM9f978QLfOtOqkOtsH1ikd7Bhtp212+MgcjRpNUqVSpUol6d9X\neECLL+3NukanrPVb9Tn2SKuUlHpTjddtl4+sT69aHSzl/sYUm312Xi590uF0EpZUa/Q5dX2YNJ+J\nVzvtlidhrBmuNcdcTUoVSqLUPIu9mnXm3WvpYB/9fxkk5yn3mG8ptcsLXrJ7SJKwpMketM/L/XSY\njtiX1pynWMFINNr/r+TMH9KsxUZvqDDRg+4xA4w3z/IBdLC6L/skQfZUeczfmmm1H/vT4N4mcxZT\nfF27d7zbz03TpSvdaOcpGVnNZ3eyzvT4/my/LuXGyVdm4mUfBx4eKi3yjNn+7AVvDqz57IdRbvS4\nIjN8Q8E5QSqlysJ0fyOp3ujh0pyQlNRzicaq2yfGmulOJYpVD/QuzCESkm7yTbOs9wu/HKKYTMIs\nt2h00A1m9ZOJJNWkLSVVKxsuzaPUKncsg6c9b/O+hUqMUnUVvDol3xTf8JDNfmyZtiHM2O/X5I+a\nL3DOHlPMHGw1yb74qzygwKcZaN5rtQ60ar4KtscVedBd2vynlYPZtHYOJW60SKefOqyn315Fh1s8\nrBj5qgbafctec6U7bc9oyKJdi1602JbB+pJcZ5S71VjhTfuH8Kw1Hlfij1b2TVX0NxyzW5N8+SoH\nqjlvAJobneqbJrsYxSrkocXWq0BzQolNltk9hPl/kVnussayi2Y6nbZoQULRQINfMut7uk6tURkt\n9ykzTj6OWjtkCcvI0ernWn0yhFGZORY54G2fX6J1HPRwb7aaazUqzvDCda5ViN1WX9ELd/8SI9/W\n4fiQrWfLU2axOyy95AsPuhzXiULTjBkezXUa5BudUWI/1c2KbbTK3qvgvWg9QxqTyXOD+bq9fcl5\n6y5HdCBf3UD7zdnG5vEmKTAzvWrkYpFstNnm6bbU64LzqfCoIr+25SLJ1xl6tfVNdiaGR/MEMxSa\nkx7GvFgv8xEPSFjuDzbHOw7PY5wl7nbc77PcoFCuZCCqs9GcMMoEExSa6na3XqTBL3WPb5lhtf/0\neQ6OTI80493nGfX22pZB6bSlG+0zmisHkpBlE5vzTTDBKO2OmexZnTZoO6+m5hvtFv9gvl1+bdmA\n145cfeQpVKREmTs87m6tkuq0auubnDi3JIvSEbmsb9lGvUZtOqX09M1DD7HmAk3qdVrnl+osUepl\nfzpnkUxStUc9pclqL3o5dkCeJa3CZLPMs9AM1ZIqLFHgY1/Y0283rUqTPBPc4j6T0p8tcMI4J7Q6\nks0Sg2za+dGe9V2l/tVy4z3kRsUO2WyLkxJSEorUW2CWLh94w6p4nMxXGuolHjLbWDVa9Sp3Zqfo\nPq9a6tPzamaNhzynUIka49GuR6mUA5p12e1tP898OW8yq0anym7rvWav3Y47YJFJqjRKKVXkhA6j\nTbDD+9625pIZ5F8XKR0O2ahbrwO61MqTkI/j/cbnhDZ7FeixVo92bXpUKpSQL8/+7Hrw2dXm72rx\nnp26ndlD1WCua0xVp9pY22x30E4rrJeI7Pq8kv7LrrMzg0uJszpMvf1E2bOPd9ajKf7y/97LpTnf\nON1azooIBUqUKFKgQFKHdl3anRzS1ctBEARBEARBEARBEARBEARBEARBEARBEATB5eP/AKOZmyQx\nnua4AAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDE5LTAzLTE3VDEzOjQwOjE3LTA3OjAw34SzHgAAACV0\nRVh0ZGF0ZTptb2RpZnkAMjAxOS0wMy0xN1QxMzo0MDoxNy0wNzowMK7ZC6IAAAAUdEVYdHBkZjpW\nZXJzaW9uAFBERi0xLjUgBVwLOQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%tikz \n",
    "\n",
    "\\node (A) {$A$};\n",
    "\\node (G) [above of=A] {$G$};\n",
    "\\node (S) [left of=A] {$S$};\n",
    "\\path[->] (G) edge (A);\n",
    "\\path[->] (S) edge (A);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity-penalized objective for the policy is thus, in its most general form:\n",
    "\n",
    "$J(\\pi) = U(A,S,G) - \\lambda_S I[A:S] - \\lambda_G I[A:G] - \\lambda_{SG} \\text{Syn}[A:G:S],$\n",
    "\n",
    "where $\\text{Syn}[A:G:S] = I[A:G|S] - I[A:G] = I[A:S|G] - I[A:S]$ is the **synergy** of $A$, $G$, and $S$, representing the amount of information that $G$ and $S$ provide jointly about $A$ above and beyond the information that each of them provides in isolation. The three $\\lambda$ parameters represent the difficulty of integrating three kinds of information: $\\lambda_S$ is the difficulty of integrating information about the speech context, $\\lambda_G$ is the difficulty of integrating information about the articulatory goal, and $\\lambda_{SG}$ is the difficulty of integrating both simultaneously.\n",
    "\n",
    "In general, intuitively, if $\\lambda_S$ is much smaller than the other $\\lambda$s, then the agent will produce sequences that have a high degree of similarity across different articulatory goals. \n",
    "\n",
    "There are other possible information processing constraints involved, and other variables, such as the fact that the agent must remember $S$ in a limited-capacity working memory. This is probably necessary to derive locality constraints in natural language. We will address the constraints later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Languages under Information Processing Constraints\n",
    "\n",
    "An optimal language will be useful for communication even when the agents that are using the language are subject to information processing constraints as described above. Considering a language as a mapping from messages $m$ to sequences of symbols $\\textbf{u}$, this mapping should not induce unnecessarily high $I[A:S]$, $I[A:G]$, and $Syn[A:G:S]$. Therefore optimal languages will have a lot of repeating substructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets of Explanation\n",
    "\n",
    "This project is somewhat open-ended in that there are a number of aspects of language we could potentially explain in this way. Once we have working models, we can figure out what are good targets of explanation.\n",
    "\n",
    "Some properties of human language that I think would be amenable to explanation in this way:\n",
    "* The set of phonemes used in any language is much smaller than the set of all pronounceable phonemes used in all languages.\n",
    "* The set of phonemes in a language has a lot of repeated substructure in terms of phonetic features.\n",
    "* The set of phonemes in a language is maximally acoustically distinct.\n",
    "* Languages usually have on the order of $10^1$ phonemes and on the order of $10^4$ morphemes: invariant sequences of phonemes which correspond to atomic components of the meaning of an utterance. \n",
    "* Morphemes vary in length (what distribution?).\n",
    "* Morphemes contain a great deal of repeated substructure (phonotactics).\n",
    "* Phonotactics and phonology are formally characterizable as $k$-tier-based strictly local languages with $k=2$.\n",
    "* Sometimes the repeated substructures convey meanings in a soft way (phonaesthemes).\n",
    "* Utterances consist of sequences of multiple morphemes (how many morphemes per sentence?).\n",
    "* Utterances vary in length.\n",
    "* The overall meaning of an utterance is compositional: it is a simple function of the meanings of the morphemes and their order.\n",
    "* There is an unbounded number of possible utterances. The cardinality of possible utterances is $\\aleph_0$.\n",
    "* Utterances have tree-like hierarchical structure: one word composes typically with one other word. That is, when the meaning of a word is context-dependent, then it is usually dependent on only one other word in the utterance. This property is called endocentricity.\n",
    "* The tree-like hierarchical structures of utterances are characterizable as Multiple Context Free Grammars with block degree $\\sim3$.\n",
    "* The tree-like hierarchical structures of utterances are characterizable as $k$-tier-based strictly local tree languages with $k=2$. (Note: I haven't carefully reviewed the literature on this, so I'm not sure that I believe it.)\n",
    "\n",
    "These are facts ranging from low-level (phonology) to high-level (syntax and semantics). I suspect that the different generalizations at these levels might be explainable in terms of different values of $\\lambda$.\n",
    "\n",
    "The numbers $10^1$ phonemes, $10^4$ morphemes, and $\\aleph_0$ sentences form a kind of hierarchy problem for linguistics. In contrast, animal communication systems have on the order of $10^1$ distinct signals with no meaningful substructure (but possibly a great deal of phonological substructure, e.g. in birdsong). DNA has $4$ \"phonemes\" (base pairs) and $4^3=64$ \"morphemes\" (codons) of fixed length (up to degeneracy of the third base pair), comprising (I think?) $\\aleph_0$ proteins of varying length.\n",
    "\n",
    "Presumably, these three systems---human language, animal communication systems, and DNA---can be recovered for different values of the $\\lambda$ parameters.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of Goals\n",
    "\n",
    "I'm arguing that the structure of language can be understood with reference to the information processing constraints that it is filtered through. Another option is that an important factor is the distribution over articulatory goals $G$ to be expressed in language, or equivalently, the distribution over messages encoded in utterances.\n",
    "\n",
    "I think it's likely that the structure of $G$ will be important. But I am not dealing with it now because I don't know how to characterize it well. I suspect that $G$ can be very generic, and that most of our linguistic structure is determined by our information processing constraints, not by the structure of $G$. My reasoning is that we don't really see natural-language-like structures anywhere in nature except in symbolic systems invented by humans (such as programming languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Approach\n",
    "\n",
    "We will represent languages as using neural networks as mappings from meanings to sequences. We will then find mappings to optimize objective functions such as those above.\n",
    "\n",
    "It would also be possible to implement information-processing constraints directly by doing things like limiting the number of hidden units in the neural networks. This might work, but we would not have a clear way of interpreting the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
